{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/labeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('data/testData.tsv', delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv('data/unlabeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "\n",
    "print('Read {} labeled train reviews, {} labeled test reviews,'\n",
    "      ' and {} unlabeled reviews'.format(train.review.size,\n",
    "                                         test.review.size,\n",
    "                                         unlabeled_train.review.size))\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pat = re.compile(r'[^a-zA-z0-9]')\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    review_text = bs(review, 'lxml').get_text()\n",
    "    review_text = pat.sub(' ', review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stops]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) == 0:\n",
    "            continue\n",
    "        sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing setntences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print('Parsing sentences from training set')\n",
    "for review in train.review:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "print('Parsing setntences from unlabeled set')\n",
    "for review in unlabeled_train.review:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-12 11:18:18,874 : INFO : loading Word2Vec object from data/300features_40minwords_10context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-12 11:18:19,231 : INFO : loading wv recursively from data/300features_40minwords_10context.wv.* with mmap=None\n",
      "2017-03-12 11:18:19,232 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-12 11:18:19,232 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-12 11:18:19,233 : INFO : loaded data/300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40  # Minimum word count\n",
    "num_workers = 6  # Number of threads to run in parallel\n",
    "context = 10  # Context window size\n",
    "downsampling = 1e-3  # Downsample setting for frequent words\n",
    "model_name = 'data/300features_40minwords_10context'\n",
    "\n",
    "if os.path.exists(model_name):\n",
    "    print('Loading Word2Vec model...')\n",
    "    model = word2vec.Word2Vec.load(model_name)\n",
    "else:\n",
    "    print('Training Word2Vec model...')\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features,\n",
    "                              min_count=min_word_count, window=context, sample=downsampling)\n",
    "    # If you don't plan to train the model any further, calling\n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-12 11:18:21,882 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6955322027206421),\n",
       " ('bride', 0.6511538028717041),\n",
       " ('maid', 0.6167219877243042),\n",
       " ('victoria', 0.5992250442504883),\n",
       " ('belle', 0.5990269184112549),\n",
       " ('mistress', 0.597449779510498),\n",
       " ('prince', 0.5903932452201843),\n",
       " ('stepmother', 0.5897886753082275),\n",
       " ('latifah', 0.5751689672470093),\n",
       " ('regina', 0.5739089250564575)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7704759836196899),\n",
       " ('atrocious', 0.7451006174087524),\n",
       " ('horrible', 0.7289239168167114),\n",
       " ('horrendous', 0.7174592018127441),\n",
       " ('abysmal', 0.711361289024353),\n",
       " ('dreadful', 0.7002981901168823),\n",
       " ('horrid', 0.689757227897644),\n",
       " ('appalling', 0.6655340194702148),\n",
       " ('lousy', 0.6393383741378784),\n",
       " ('amateurish', 0.6360909938812256)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('awful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype='float32')\n",
    "    \n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if nwords:\n",
    "        feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features),\n",
    "                                   dtype='float32')\n",
    "    \n",
    "    for counter, review in enumerate(reviews):\n",
    "        if counter % 1000 == 0:\n",
    "            print('Review {} of {}'.format(counter, len(reviews)))\n",
    "        \n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, \n",
    "                                                        num_features)\n",
    "    \n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for train reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "print('Creating average feature vecs for train reviews')\n",
    "clean_train_reviews = []\n",
    "for review in train.review:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, \n",
    "                                                  remove_stopwords=False))\n",
    "    \n",
    "train_data_vecs = get_avg_feature_vecs(clean_train_reviews, model,\n",
    "                                       num_features)\n",
    "\n",
    "print('Creating average feature vecs for test reviews')\n",
    "clean_test_reviews = []\n",
    "for review in test.review:\n",
    "    clean_test_reviews.append(review_to_wordlist(review,\n",
    "                                                 remove_stopwords=False))\n",
    "\n",
    "test_data_vecs = get_avg_feature_vecs(clean_test_reviews, model,\n",
    "                                      num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "clf = clf.fit(train_data_vecs, train.sentiment)\n",
    "h = clf.predict(test_data_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = pd.DataFrame({'id': test.id, 'sentiment': h})\n",
    "ans.to_csv('data/submit.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering: 415.641\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print('Time taken for K Means clustering: {:.3f}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "['roberta', 'malibu', 'dell']\n",
      "Cluster 1\n",
      "['population', 'outsiders', 'nukie', 'populace', 'ownership']\n",
      "Cluster 2\n",
      "['delivered']\n",
      "Cluster 3\n",
      "['wheelchair', 'photograph', 'brothel', 'convent', 'villa', 'workplace', 'detention']\n",
      "Cluster 4\n",
      "['perfect', 'flawless', 'commendable']\n",
      "Cluster 5\n",
      "['francis', 'ross', 'shelley', 'minnelli', 'irving', 'crosby', 'judd', 'bing', 'burnett', 'jeanette', 'lanza']\n",
      "Cluster 6\n",
      "['pictures', 'classics', 'productions', 'westerns', 'musicals', 'masterpieces', 'remakes', 'epics', 'serials', 'noirs', 'melodramas']\n",
      "Cluster 7\n",
      "['recorded', 'taped', 'switched']\n",
      "Cluster 8\n",
      "['psycho', 'freak', 'maniac', 'madman', 'biker', 'redneck', 'scarecrow', 'leatherface', 'lunatic', 'bloke', 'weirdo', 'grizzly']\n",
      "Cluster 9\n",
      "['joyous', 'giddy']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(10):\n",
    "    print('Cluster {}'.format(cluster))\n",
    "    \n",
    "    words = [word for word, idx in word_centroid_map.items()\n",
    "             if idx == cluster]\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(word_list, word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype='float32')\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create bag of centroids for train set: 0 / 25000\n",
      "Create bag of centroids for train set: 1000 / 25000\n",
      "Create bag of centroids for train set: 2000 / 25000\n",
      "Create bag of centroids for train set: 3000 / 25000\n",
      "Create bag of centroids for train set: 4000 / 25000\n",
      "Create bag of centroids for train set: 5000 / 25000\n",
      "Create bag of centroids for train set: 6000 / 25000\n",
      "Create bag of centroids for train set: 7000 / 25000\n",
      "Create bag of centroids for train set: 8000 / 25000\n",
      "Create bag of centroids for train set: 9000 / 25000\n",
      "Create bag of centroids for train set: 10000 / 25000\n",
      "Create bag of centroids for train set: 11000 / 25000\n",
      "Create bag of centroids for train set: 12000 / 25000\n",
      "Create bag of centroids for train set: 13000 / 25000\n",
      "Create bag of centroids for train set: 14000 / 25000\n",
      "Create bag of centroids for train set: 15000 / 25000\n",
      "Create bag of centroids for train set: 16000 / 25000\n",
      "Create bag of centroids for train set: 17000 / 25000\n",
      "Create bag of centroids for train set: 18000 / 25000\n",
      "Create bag of centroids for train set: 19000 / 25000\n",
      "Create bag of centroids for train set: 20000 / 25000\n",
      "Create bag of centroids for train set: 21000 / 25000\n",
      "Create bag of centroids for train set: 22000 / 25000\n",
      "Create bag of centroids for train set: 23000 / 25000\n",
      "Create bag of centroids for train set: 24000 / 25000\n",
      "Create bag of centroids for test set: 0 / 25000\n",
      "Create bag of centroids for test set: 1000 / 25000\n",
      "Create bag of centroids for test set: 2000 / 25000\n",
      "Create bag of centroids for test set: 3000 / 25000\n",
      "Create bag of centroids for test set: 4000 / 25000\n",
      "Create bag of centroids for test set: 5000 / 25000\n",
      "Create bag of centroids for test set: 6000 / 25000\n",
      "Create bag of centroids for test set: 7000 / 25000\n",
      "Create bag of centroids for test set: 8000 / 25000\n",
      "Create bag of centroids for test set: 9000 / 25000\n",
      "Create bag of centroids for test set: 10000 / 25000\n",
      "Create bag of centroids for test set: 11000 / 25000\n",
      "Create bag of centroids for test set: 12000 / 25000\n",
      "Create bag of centroids for test set: 13000 / 25000\n",
      "Create bag of centroids for test set: 14000 / 25000\n",
      "Create bag of centroids for test set: 15000 / 25000\n",
      "Create bag of centroids for test set: 16000 / 25000\n",
      "Create bag of centroids for test set: 17000 / 25000\n",
      "Create bag of centroids for test set: 18000 / 25000\n",
      "Create bag of centroids for test set: 19000 / 25000\n",
      "Create bag of centroids for test set: 20000 / 25000\n",
      "Create bag of centroids for test set: 21000 / 25000\n",
      "Create bag of centroids for test set: 22000 / 25000\n",
      "Create bag of centroids for test set: 23000 / 25000\n",
      "Create bag of centroids for test set: 24000 / 25000\n"
     ]
    }
   ],
   "source": [
    "train_centroids = np.zeros((train.review.size, num_clusters),\n",
    "                            dtype='float32')\n",
    "\n",
    "for idx, review in enumerate(clean_train_reviews):\n",
    "    if idx % 1000 == 0:\n",
    "        print('Create bag of centroids for train set: {} / {}'.format(\n",
    "            idx, len(clean_train_reviews)))\n",
    "    train_centroids[idx] = create_bag_of_centroids(review,\n",
    "                                                   word_centroid_map)\n",
    "\n",
    "test_centroids = np.zeros((test.review.size, num_clusters),\n",
    "                          dtype='float32')\n",
    "\n",
    "for idx, review in enumerate(clean_test_reviews):\n",
    "    if idx % 1000 == 0:\n",
    "        print('Create bag of centroids for test set: {} / {}'.format(\n",
    "            idx, len(clean_test_reviews)))\n",
    "    test_centroids[idx] = create_bag_of_centroids(review,\n",
    "                                                  word_centroid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "print('Fitting a random forest to labeled training data...')\n",
    "clf.fit(train_centroids, train.sentiment)\n",
    "h = clf.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = pd.DataFrame({'id': test.id, 'sentiment': h})\n",
    "ans.to_csv('data/submit.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
